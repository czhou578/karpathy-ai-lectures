{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# makemore: part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nat beginning, all character probs should be equal. Otherwise, nn will be confidently wrong, high loss.\\n\\ndon't set weights exactly to 0. why? \\n\\nweights and biases have to be gaussian.\\n\\n.tolist() - convert into one large python list\\ntanh being 0 can stop the backprop (look at micrograd lecture) \\n    -> (always 1 or -1, will not learn since gradient is zeroed out)\\n\\nsquash weights -> get rid of dead neurons: no saturation of neurons in tanh.\\n\\nunit gaussian - divide by number of sqrt(num of inputs for weight), this is what we want\\n\\nBefore, we had to be careful of gradients and be very precise with settings.\\n\\nkaiming init: calculate 'gain', why need gain? fight squeeze in of tanh, boost weights a little to renormalize it to std dev.\\n\\nbatch normalization: made training deep NN's very reliably\\n    - want hidden states to be gaussian, why don't we initalize them that way??\\n    - append batch norm. layer after linear / convo layer, controls scale of activations\\n    - scale and shift (taking normalized input, scale by a gain and offset by bias)\\n    - want to have unit gaussian for each neuron\\n    - examples coupled together ( creates entropy as a side effect)\\n    - take activations, mean, std, and center data (center operation is differentiable)\\n    - gain and bias are now trainable!\\n    - to do inference, estimate mean and std dev once for entire set (do that in running)\\n    - coupling examples in fws pass (causes a lot of bugs, watch out!)\\n    \\n    estimate mean and std dev in a running manner as training is happening\\n\\nbatch norm. introduces noise because the logits of a single training example is now being affected by the \\npresence of whichever other examples are in the same batch. \\n\\nweight layer\\nnormalization\\nnonlinearity\\n\\nbatch layer deletes bias and adds own bias. just delete b1.\\nsprinkle batch layer throughout network, typically after multiplicative layers\\n\\nparameter, buffer of batchnorm. \\n\\nif too confident predictions, losses will have hockey stick shape\\ncontrol activations (not too low, or exploding to infinity), want gaussian activations throughout nn, roughly homogeneous\\n\\nupdate-data ratio: determine how much data to change in tensors.\\n\\nstacking linear layers, end up with one linear transformation (that's why we need tanh)\\nupdate to data ratio is what matters ()\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "You should know what loss to expect from the beginning of network initialization\n",
    "\n",
    "at beginning, all character probs should be equal. Otherwise, nn will be confidently wrong, high loss.\n",
    "\n",
    "don't set weights exactly to 0. why? \n",
    "\n",
    "weights and biases have to be gaussian.\n",
    "\n",
    ".tolist() - convert into one large python list\n",
    "tanh being 0 can stop the backprop (look at micrograd lecture) \n",
    "    -> (always 1 or -1, will not learn since gradient is zeroed out)\n",
    "\n",
    "squash weights -> get rid of dead neurons: no saturation of neurons in tanh.\n",
    "\n",
    "unit gaussian - divide by number of sqrt(num of inputs for weight), this is what we want\n",
    "\n",
    "Before, we had to be careful of gradients and be very precise with settings.\n",
    "\n",
    "kaiming init: calculate 'gain', why need gain? fight squeeze in of tanh, boost weights a little to renormalize it to std dev.\n",
    "\n",
    "batch normalization: made training deep NN's very reliably\n",
    "    - want hidden states to be gaussian, why don't we initalize them that way??\n",
    "    - append batch norm. layer after linear / convo layer, controls scale of activations\n",
    "    - scale and shift (taking normalized input, scale by a gain and offset by bias)\n",
    "    - want to have unit gaussian for each neuron\n",
    "    - examples coupled together ( creates entropy as a side effect)\n",
    "    - take activations, mean, std, and center data (center operation is differentiable)\n",
    "    - gain and bias are now trainable!\n",
    "    - to do inference, estimate mean and std dev once for entire set (do that in running)\n",
    "    - coupling examples in fws pass (causes a lot of bugs, watch out!)\n",
    "    \n",
    "    estimate mean and std dev in a running manner as training is happening\n",
    "\n",
    "batch norm. introduces noise because the logits of a single training example is now being affected by the \n",
    "presence of whichever other examples are in the same batch. \n",
    "\n",
    "weight layer\n",
    "normalization\n",
    "nonlinearity\n",
    "\n",
    "batch layer deletes bias and adds own bias. just delete b1.\n",
    "sprinkle batch layer throughout network, typically after multiplicative layers\n",
    "\n",
    "parameter, buffer of batchnorm. \n",
    "\n",
    "if too confident predictions, losses will have hockey stick shape\n",
    "control activations (not too low, or exploding to infinity), want gaussian activations throughout nn, roughly homogeneous\n",
    "\n",
    "update-data ratio: determine how much data to change in tensors.\n",
    "\n",
    "stacking linear layers, end up with one linear transformation (that's why we need tanh)\n",
    "update to data ratio is what matters ()\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n"
     ]
    }
   ],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[0;32m     33\u001b[0m   p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# update\u001b[39;00m\n\u001b[0;32m     37\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.01\u001b[39m \u001b[38;5;66;03m# step learning rate decay\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mycol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mycol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  with torch.no_grad():\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0f056ef70>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 # + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnstd = hpreact.std(0, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.552957773208618\n",
      "val 2.5584352016448975\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 # + b1\n",
    "  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss log\n",
    "\n",
    "### original:\n",
    "train 2.1245384216308594\n",
    "val   2.168196439743042\n",
    "\n",
    "### fix softmax confidently wrong:\n",
    "train 2.07\n",
    "val   2.13\n",
    "\n",
    "### fix tanh layer too saturated at init:\n",
    "train 2.0355966091156006\n",
    "val   2.1026785373687744\n",
    "\n",
    "### use semi-principled \"kaiming init\" instead of hacky init:\n",
    "train 2.0376641750335693\n",
    "val   2.106989622116089\n",
    "\n",
    "### add batch norm layer\n",
    "train 2.0668270587921143\n",
    "val 2.104844808578491\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY + PYTORCHIFYING -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47024\n"
     ]
    }
   ],
   "source": [
    "# Let's train a deeper network\n",
    "# The classes we create here are the same API as nn.Module in PyTorch\n",
    "\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    # self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdim=True) # batch mean\n",
    "      xvar = x.var(0, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "# layers = [\n",
    "#   Linear(n_embd * block_size, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, vocab_size),\n",
    "# ]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "  layers[-1].gamma *= 0.1\n",
    "  #layers[-1].weight *= 0.1\n",
    "  # all other layers: apply gain\n",
    "  for layer in layers[:-1]:\n",
    "    if isinstance(layer, Linear):\n",
    "      layer.weight *= 1.0 #5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "can't retain_grad on Tensor that has requires_grad=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[1;32m---> 22\u001b[0m   \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretain_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# AFTER_DEBUG: would take out retain_graph\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[0;32m     24\u001b[0m   p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: can't retain_grad on Tensor that has requires_grad=False"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for layer in layers:\n",
    "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  with torch.no_grad():\n",
    "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "  if i >= 1000:\n",
    "    break # AFTER_DEBUG: would take out obviously to run full optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tanh' object has no attribute 'out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(layers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]): \u001b[38;5;66;03m# note: exclude the output layer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Tanh):\n\u001b[1;32m----> 6\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%10s\u001b[39;00m\u001b[38;5;124m): mean \u001b[39m\u001b[38;5;132;01m%+.2f\u001b[39;00m\u001b[38;5;124m, std \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m, saturated: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (i, layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, t\u001b[38;5;241m.\u001b[39mmean(), t\u001b[38;5;241m.\u001b[39mstd(), (t\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.97\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m))\n\u001b[0;32m      8\u001b[0m     hy, hx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhistogram(t, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tanh' object has no attribute 'out'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tanh' object has no attribute 'out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(layers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]): \u001b[38;5;66;03m# note: exclude the output layer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Tanh):\n\u001b[1;32m----> 6\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[38;5;241m.\u001b[39mgrad\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%10s\u001b[39;00m\u001b[38;5;124m): mean \u001b[39m\u001b[38;5;132;01m%+f\u001b[39;00m\u001b[38;5;124m, std \u001b[39m\u001b[38;5;132;01m%e\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (i, layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, t\u001b[38;5;241m.\u001b[39mmean(), t\u001b[38;5;241m.\u001b[39mstd()))\n\u001b[0;32m      8\u001b[0m     hy, hx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhistogram(t, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tanh' object has no attribute 'out'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out.grad\n",
    "    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('gradient distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m t \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mgrad\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight \u001b[39m\u001b[38;5;132;01m%10s\u001b[39;00m\u001b[38;5;124m | mean \u001b[39m\u001b[38;5;132;01m%+f\u001b[39;00m\u001b[38;5;124m | std \u001b[39m\u001b[38;5;132;01m%e\u001b[39;00m\u001b[38;5;124m | grad:data ratio \u001b[39m\u001b[38;5;132;01m%e\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtuple\u001b[39m(p\u001b[38;5;241m.\u001b[39mshape), \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m(), t\u001b[38;5;241m.\u001b[39mstd(), t\u001b[38;5;241m.\u001b[39mstd() \u001b[38;5;241m/\u001b[39m p\u001b[38;5;241m.\u001b[39mstd()))\n\u001b[0;32m      8\u001b[0m   hy, hx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhistogram(t, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m   plt\u001b[38;5;241m.\u001b[39mplot(hx[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach(), hy\u001b[38;5;241m.\u001b[39mdetach())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'mean'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  t = p.grad\n",
    "  if p.ndim == 2:\n",
    "    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends)\n",
    "plt.title('weights gradient distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(parameters):\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m----> 5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot([ud[j][i] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mud\u001b[49m))])\n\u001b[0;32m      6\u001b[0m     legends\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparam \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m i)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ud)], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# these ratios should be ~1e-3, indicate on plot\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ud' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  if p.ndim == 2:\n",
    "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "    legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "plt.legend(legends);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.4002976417541504\n",
      "val 2.3982467651367188\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "# put layers into eval mode\n",
    "for layer in layers:\n",
    "  layer.training = False\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carpah.\n",
      "qarlileif.\n",
      "jmrix.\n",
      "thty.\n",
      "sacansa.\n",
      "jazhnte.\n",
      "dpn.\n",
      "arciigqeiunellaia.\n",
      "chriiv.\n",
      "kalein.\n",
      "dhlm.\n",
      "join.\n",
      "qhinn.\n",
      "sroin.\n",
      "arian.\n",
      "quiqaelogiearyxix.\n",
      "kaeklinsan.\n",
      "ed.\n",
      "ecoia.\n",
      "gtleley.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "      for layer in layers:\n",
    "        x = layer(x)\n",
    "      logits = x\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      # shift the context window and track the samples\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE; BONUS content below, not covered in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f06d28493c14ed4ad68c5463a28e34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='x0', max=30.0, min=-30.0, step=0.5), Output()), _dom…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BatchNorm forward pass as a widget\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "def normshow(x0):\n",
    "  \n",
    "  g = torch.Generator().manual_seed(2147483647+1)\n",
    "  x = torch.randn(5, generator=g) * 5\n",
    "  x[0] = x0 # override the 0th example with the slider\n",
    "  mu = x.mean()\n",
    "  sig = x.std()\n",
    "  y = (x - mu)/sig\n",
    "\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  # plot 0\n",
    "  plt.plot([-6,6], [0,0], 'k')\n",
    "  # plot the mean and std\n",
    "  xx = np.linspace(-6, 6, 100)\n",
    "  plt.plot(xx, stats.norm.pdf(xx, mu, sig), 'b')\n",
    "  xx = np.linspace(-6, 6, 100)\n",
    "  plt.plot(xx, stats.norm.pdf(xx, 0, 1), 'r')\n",
    "  # plot little lines connecting input and output\n",
    "  for i in range(len(x)):\n",
    "    plt.plot([x[i],y[i]], [1, 0], 'k', alpha=0.2)\n",
    "  # plot the input and output values\n",
    "  plt.scatter(x.data, torch.ones_like(x).data, c='b', s=100)\n",
    "  plt.scatter(y.data, torch.zeros_like(y).data, c='r', s=100)\n",
    "  plt.xlim(-6, 6)\n",
    "  # title\n",
    "  plt.title('input mu %.2f std %.2f' % (mu, sig))\n",
    "\n",
    "interact(normshow, x0=(-30,30,0.5));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a std: 0.9875972270965576\n",
      "b std: 1.0006722211837769\n",
      "c std: 31.01241683959961\n",
      "-----\n",
      "c grad std: 0.9782556295394897\n",
      "a grad std: 30.8818302154541\n",
      "b grad std: 0.9666601419448853\n"
     ]
    }
   ],
   "source": [
    "# Linear: activation statistics of forward and backward pass\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "a = torch.randn((1000,1), requires_grad=True, generator=g)          # a.grad = b.T @ c.grad\n",
    "b = torch.randn((1000,1000), requires_grad=True, generator=g)       # b.grad = c.grad @ a.T\n",
    "c = b @ a\n",
    "loss = torch.randn(1000, generator=g) @ c\n",
    "a.retain_grad()\n",
    "b.retain_grad()\n",
    "c.retain_grad()\n",
    "loss.backward()\n",
    "print('a std:', a.std().item())\n",
    "print('b std:', b.std().item())\n",
    "print('c std:', c.std().item())\n",
    "print('-----')\n",
    "print('c grad std:', c.grad.std().item())\n",
    "print('a grad std:', a.grad.std().item())\n",
    "print('b grad std:', b.grad.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp std:  0.9875972270965576\n",
      "w std:  1.0006722211837769\n",
      "x std:  31.01241683959961\n",
      "out std:  1.0\n",
      "------\n",
      "out grad std:  0.9782556295394897\n",
      "x grad std:  0.031543977558612823\n",
      "w grad std:  0.031169468536973\n",
      "inp grad std:  0.9953052997589111\n"
     ]
    }
   ],
   "source": [
    "# Linear + BatchNorm: activation statistics of forward and backward pass\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "n = 1000\n",
    "# linear layer ---\n",
    "inp = torch.randn(n, requires_grad=True, generator=g)\n",
    "w = torch.randn((n, n), requires_grad=True, generator=g) # / n**0.5\n",
    "x = w @ inp\n",
    "# bn layer ---\n",
    "xmean = x.mean()\n",
    "xvar = x.var()\n",
    "out = (x - xmean) / torch.sqrt(xvar + 1e-5)\n",
    "# ----\n",
    "loss = out @ torch.randn(n, generator=g)\n",
    "inp.retain_grad()\n",
    "x.retain_grad()\n",
    "w.retain_grad()\n",
    "out.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "print('inp std: ', inp.std().item())\n",
    "print('w std: ', w.std().item())\n",
    "print('x std: ', x.std().item())\n",
    "print('out std: ', out.std().item())\n",
    "print('------')\n",
    "print('out grad std: ', out.grad.std().item())\n",
    "print('x grad std: ', x.grad.std().item())\n",
    "print('w grad std: ', w.grad.std().item())\n",
    "print('inp grad std: ', inp.grad.std().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
