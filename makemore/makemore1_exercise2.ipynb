{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28829\n",
      "torch.Size([25626])\n",
      "tensor([25, 14, 14,  ..., 19,  5, 20])\n",
      "tensor([20,  1,  0,  ..., 25, 26, 24])\n"
     ]
    }
   ],
   "source": [
    "# Exercises 2: E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "words_len = len(words)\n",
    "train_idx = int(0.80 * words_len)\n",
    "dev_idx = int(0.90 * words_len)\n",
    "print(dev_idx)\n",
    "\n",
    "xtrain, ytrain = xs[:train_idx], ys[:train_idx]\n",
    "xdev, ydev = xs[train_idx:dev_idx], ys[train_idx:dev_idx]\n",
    "xtest, ytest = xs[dev_idx:], ys[dev_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8023996353149414\n",
      "3.7999112606048584\n",
      "3.7974281311035156\n",
      "3.794949769973755\n",
      "3.7924771308898926\n",
      "3.7900094985961914\n",
      "3.7875471115112305\n",
      "3.7850897312164307\n",
      "3.782637596130371\n",
      "3.7801902294158936\n",
      "3.7777483463287354\n",
      "3.77531099319458\n",
      "3.7728793621063232\n",
      "3.7704522609710693\n",
      "3.7680304050445557\n",
      "3.7656140327453613\n",
      "3.7632017135620117\n",
      "3.7607951164245605\n",
      "3.7583930492401123\n",
      "3.7559962272644043\n",
      "3.75360369682312\n",
      "3.7512171268463135\n",
      "3.7488350868225098\n",
      "3.746457576751709\n",
      "3.7440850734710693\n",
      "3.74171781539917\n",
      "3.7393550872802734\n",
      "3.736997127532959\n",
      "3.7346444129943848\n",
      "3.7322962284088135\n",
      "3.7299530506134033\n",
      "3.727614641189575\n",
      "3.72528076171875\n",
      "3.722951650619507\n",
      "3.720627546310425\n",
      "3.7183079719543457\n",
      "3.7159934043884277\n",
      "3.713683605194092\n",
      "3.7113780975341797\n",
      "3.709077835083008\n",
      "3.7067816257476807\n",
      "3.7044906616210938\n",
      "3.702204942703247\n",
      "3.6999223232269287\n",
      "3.697645902633667\n",
      "3.695373773574829\n",
      "3.693105697631836\n",
      "3.690842628479004\n",
      "3.688584327697754\n",
      "3.6863303184509277\n",
      "3.6840810775756836\n",
      "3.6818366050720215\n",
      "3.6795966625213623\n",
      "3.677361249923706\n",
      "3.6751296520233154\n",
      "3.6729037761688232\n",
      "3.670681953430176\n",
      "3.6684648990631104\n",
      "3.666252851486206\n",
      "3.6640443801879883\n",
      "3.66184139251709\n",
      "3.659641981124878\n",
      "3.657447576522827\n",
      "3.6552581787109375\n",
      "3.6530728340148926\n",
      "3.6508922576904297\n",
      "3.6487154960632324\n",
      "3.6465442180633545\n",
      "3.644376754760742\n",
      "3.6422135829925537\n",
      "3.640054941177368\n",
      "3.637901544570923\n",
      "3.635751962661743\n",
      "3.6336069107055664\n",
      "3.6314666271209717\n",
      "3.629330635070801\n",
      "3.627199649810791\n",
      "3.6250720024108887\n",
      "3.6229491233825684\n",
      "3.620831251144409\n",
      "3.6187174320220947\n",
      "3.616607666015625\n",
      "3.6145033836364746\n",
      "3.6124024391174316\n",
      "3.61030650138855\n",
      "3.608215093612671\n",
      "3.6061272621154785\n",
      "3.6040444374084473\n",
      "3.601966142654419\n",
      "3.5998919010162354\n",
      "3.597822666168213\n",
      "3.595757007598877\n",
      "3.593696355819702\n",
      "3.591639757156372\n",
      "3.589587450027466\n",
      "3.5875399112701416\n",
      "3.585496425628662\n",
      "3.5834577083587646\n",
      "3.581422805786133\n",
      "3.579392910003662\n",
      "Mean of the last 10 training loss:  3.5885812997817994\n",
      "Mean of the last 10 dev set loss:  3.5748335123062134\n"
     ]
    }
   ],
   "source": [
    "training_loss_arr = []\n",
    "dev_loss_arr = []\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "for k in range(100):\n",
    "    xenc = F.one_hot(xtrain, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(25626), ytrain].log().mean() + 0.01 * (W**2).mean() \n",
    "    print(loss.item())\n",
    "\n",
    "    if k >= 90:\n",
    "        with torch.no_grad():\n",
    "            xenc = F.one_hot(xdev, num_classes=27).float()\n",
    "            logits_dev = xenc @ W\n",
    "            dev_counts = logits_dev.exp()\n",
    "            probs = dev_counts / dev_counts.sum(1, keepdims=True)\n",
    "            dev_loss = -probs[torch.arange(ydev.shape[0]), ydev].log().mean()\n",
    "\n",
    "        training_loss_arr.append(loss.item())\n",
    "        dev_loss_arr.append(dev_loss.item())\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -0.2 * W.grad #\n",
    "\n",
    "print(\"Mean of the last 10 training loss: \", sum(training_loss_arr)/10)\n",
    "print(\"Mean of the last 10 dev set loss: \", sum(dev_loss_arr)/10)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the last 10 test set loss:  0.35768148899078367\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k in range(10):\n",
    "        xenc = F.one_hot(xtest, num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        test_counts = logits.exp()\n",
    "        test_probs = test_counts / test_counts.sum(1, keepdims=True)\n",
    "        loss = -test_probs[torch.arange(ytest.shape[0]), ytest].log().mean()\n",
    "\n",
    "    test_losses.append(loss.item())\n",
    "\n",
    "print(\"Mean of the last 10 test set loss: \", sum(test_losses) / 10)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 13])\n",
      "tensor([13, 13,  1,  ..., 26, 24,  0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25626"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append([ix1, ix2])\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "words_len = len(words)\n",
    "train_idx = int(0.80 * words_len)\n",
    "dev_idx = int(0.90 * words_len)\n",
    "\n",
    "print(xs[2])\n",
    "print(ys)\n",
    "\n",
    "\n",
    "xtrain, ytrain = xs[:train_idx], ys[:train_idx]\n",
    "xdev, ydev = xs[train_idx:dev_idx], ys[train_idx:dev_idx]\n",
    "xtest, ytest = xs[dev_idx:], ys[dev_idx:]\n",
    "\n",
    "ytrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7455999851226807\n",
      "3.399603843688965\n",
      "3.1978631019592285\n",
      "3.079557180404663\n",
      "3.002737045288086\n",
      "2.9481465816497803\n",
      "2.906745195388794\n",
      "2.873762607574463\n",
      "2.846700429916382\n",
      "2.824082136154175\n",
      "2.804903984069824\n",
      "2.7884368896484375\n",
      "2.7741386890411377\n",
      "2.7616047859191895\n",
      "2.75052809715271\n",
      "2.7406723499298096\n",
      "2.7318527698516846\n",
      "2.723921775817871\n",
      "2.7167587280273438\n",
      "2.710265636444092\n",
      "2.7043609619140625\n",
      "2.698974847793579\n",
      "2.6940503120422363\n",
      "2.6895360946655273\n",
      "2.6853890419006348\n",
      "2.681570053100586\n",
      "2.6780457496643066\n",
      "2.6747865676879883\n",
      "2.671764612197876\n",
      "2.668957471847534\n",
      "2.666344404220581\n",
      "2.6639068126678467\n",
      "2.661628246307373\n",
      "2.659494161605835\n",
      "2.657491445541382\n",
      "2.655609130859375\n",
      "2.653837203979492\n",
      "2.6521661281585693\n",
      "2.650588035583496\n",
      "2.649095296859741\n",
      "2.647681951522827\n",
      "2.6463425159454346\n",
      "2.6450705528259277\n",
      "2.6438615322113037\n",
      "2.6427114009857178\n",
      "2.641616106033325\n",
      "2.6405720710754395\n",
      "2.6395750045776367\n",
      "2.6386239528656006\n",
      "2.63771390914917\n",
      "2.63684344291687\n",
      "2.636009931564331\n",
      "2.635211229324341\n",
      "2.6344454288482666\n",
      "2.6337103843688965\n",
      "2.6330041885375977\n",
      "2.6323258876800537\n",
      "2.631673574447632\n",
      "2.6310458183288574\n",
      "2.630441188812256\n",
      "2.62985897064209\n",
      "2.6292974948883057\n",
      "2.628756284713745\n",
      "2.6282336711883545\n",
      "2.6277294158935547\n",
      "2.62724232673645\n",
      "2.6267709732055664\n",
      "2.6263158321380615\n",
      "2.6258749961853027\n",
      "2.625448703765869\n",
      "2.625035285949707\n",
      "2.624635696411133\n",
      "2.6242477893829346\n",
      "2.6238725185394287\n",
      "2.6235079765319824\n",
      "2.6231536865234375\n",
      "2.6228103637695312\n",
      "2.6224775314331055\n",
      "2.6221530437469482\n",
      "2.6218388080596924\n",
      "2.621532678604126\n",
      "2.6212356090545654\n",
      "2.620946168899536\n",
      "2.6206648349761963\n",
      "2.6203906536102295\n",
      "2.620123863220215\n",
      "2.6198642253875732\n",
      "2.6196112632751465\n",
      "2.6193642616271973\n",
      "2.619123935699463\n",
      "2.618889570236206\n",
      "2.6186609268188477\n",
      "2.6184377670288086\n",
      "2.618220329284668\n",
      "2.6180076599121094\n",
      "2.617800235748291\n",
      "2.617597818374634\n",
      "2.6173996925354004\n",
      "2.6172068119049072\n",
      "2.6170177459716797\n"
     ]
    }
   ],
   "source": [
    "avg_dev_loss = []\n",
    "avg_train_loss = []\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "for k in range(100):\n",
    "    xenc = F.one_hot(xtrain, num_classes=27).float()\n",
    "    logits = xenc.view(-1, 27) @ W\n",
    "\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    # print(probs.shape)\n",
    "    loss = -probs[torch.arange(ytrain.shape[0]), ytrain].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    if k >= 90:\n",
    "        with torch.no_grad():\n",
    "            xdevenc = F.one_hot(xdev, num_classes=27).float()\n",
    "            dev_logits = xdevenc.view(-1, 27) @ W\n",
    "            dev_counts = dev_logits.exp()\n",
    "            dev_probs = dev_counts / dev_counts.sum(1, keepdims=True)\n",
    "            dev_loss = -probs[torch.arange(ydev.shape[0]), ydev].log().mean()\n",
    "        \n",
    "        avg_dev_loss.append(dev_loss)\n",
    "        avg_train_loss.append(loss)\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -50 * W.grad #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss for trigram is,  0.2746225357055664\n"
     ]
    }
   ],
   "source": [
    "trigram_test_loss = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k in range(10):\n",
    "        test_vec = F.one_hot(xtest, num_classes=27).float()\n",
    "        test_logits = test_vec.view(-1, 27) @ W\n",
    "        test_counts = test_logits.exp()\n",
    "        test_probs = test_counts / test_counts.sum(1, keepdims=True)\n",
    "        test_loss = -test_probs[torch.arange(ytest.shape[0]), ytest].log().mean()\n",
    "\n",
    "    trigram_test_loss.append(test_loss.item())\n",
    "\n",
    "print(\"Test loss for trigram is, \", sum(trigram_test_loss) / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "unidn.\n",
      "ianagaz.\n",
      "p.\n",
      "ofaywoinn.\n"
     ]
    }
   ],
   "source": [
    "#sampling from the trigram model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "\n",
    "for k in range(5):\n",
    "    out = []\n",
    "    idx = 0\n",
    "\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([idx]), num_classes=27).float()\n",
    "        logits = xenc.view(-1, 27) @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0: \n",
    "            break\n",
    "\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the training loss for r = 1:  5.044561343722873\n",
      "Mean of the dev set loss for r = 1:  3.5639166593551637\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.5:  4.31432045035892\n",
      "Mean of the dev set loss for r = 0.5:  3.4287187337875364\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.1:  3.827811532550388\n",
      "Mean of the dev set loss for r = 0.1:  3.326589322090149\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.05:  3.6839466624789767\n",
      "Mean of the dev set loss for r = 0.05:  3.2435006380081175\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.01:  3.569041363398234\n",
      "Mean of the dev set loss for r = 0.01:  3.175637197494507\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.005:  3.4972011937035457\n",
      "Mean of the dev set loss for r = 0.005:  3.1196708202362062\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.001:  3.437793162133959\n",
      "Mean of the dev set loss for r = 0.001:  3.0729807138442995\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.0005:  3.390081442726983\n",
      "Mean of the dev set loss for r = 0.0005:  3.033469295501709\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.0001:  3.3494296153386434\n",
      "Mean of the dev set loss for r = 0.0001:  2.9995779514312746\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 5e-05:  3.314558023876614\n",
      "Mean of the dev set loss for r = 5e-05:  2.9701598405838014\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 1e-05:  3.2841192960739134\n",
      "Mean of the dev set loss for r = 1e-05:  2.9443591117858885\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "regs=[1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "for r in regs:\n",
    "    training_loss_arr = []\n",
    "    dev_loss_arr = []\n",
    "\n",
    "    for k in range(100):\n",
    "        xenc = F.one_hot(xdev, num_classes=27).float()\n",
    "        logits = xenc.view(-1, 27) @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        loss = -probs[torch.arange(ydev.shape[0]), ydev].log().mean() + r * (W**2).mean() \n",
    "        # print(loss.item())\n",
    "\n",
    "        if k >= 90:\n",
    "            with torch.no_grad():\n",
    "                xenc = F.one_hot(xdev, num_classes=27).float()\n",
    "                logits_dev = xenc.view(-1, 27) @ W\n",
    "                dev_counts = logits_dev.exp()\n",
    "                probs = dev_counts / dev_counts.sum(1, keepdims=True)\n",
    "                dev_loss = -probs[torch.arange(ydev.shape[0]), ydev].log().mean()\n",
    "            dev_loss_arr.append(dev_loss.item())\n",
    "\n",
    "        training_loss_arr.append(loss.item())\n",
    "\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        W.data += -0.2 * W.grad #\n",
    "\n",
    "    print(f\"Mean of the training loss for r = {r}: \", sum(training_loss_arr)/90)\n",
    "    print(f\"Mean of the dev set loss for r = {r}: \", sum(dev_loss_arr)/10)   \n",
    "    print(f'----------------------------------------------------------------') \n",
    "\n",
    "#If the weight is too low, then there will be a loss, but if weight is too high, too much smoothing also increases loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25626, 2])\n",
      "Mean of the training loss for r = 1:  4.08489236301846\n",
      "Mean of the dev set loss for r = 1:  3.6042278528213503\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.5:  3.937594183286031\n",
      "Mean of the dev set loss for r = 0.5:  3.483780026435852\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.1:  3.8148920747968886\n",
      "Mean of the dev set loss for r = 0.1:  3.383339262008667\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.05:  3.7131011750963\n",
      "Mean of the dev set loss for r = 0.05:  3.3001503944396973\n",
      "----------------------------------------------------------------\n",
      "Mean of the training loss for r = 0.01:  3.6291097005208335\n",
      "Mean of the dev set loss for r = 0.01:  3.231557273864746\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[197], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     training_loss_arr\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     30\u001b[0m     W\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     W\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m W\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean of the training loss for r = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(training_loss_arr)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m90\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mycol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mycol\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# regs=[1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "print(xtrain.shape)\n",
    "for k in range(100):\n",
    "    xenc = F.one_hot(xtrain, num_classes=27).float()\n",
    "    logits = xenc.view(-1, 27) @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(ytrain.shape[0]), ytrain].log().mean() + 0.005 * (W**2).mean() \n",
    "\n",
    "    if k >= 90:\n",
    "        with torch.no_grad():\n",
    "            xenc = F.one_hot(xdev, num_classes=27).float()\n",
    "            logits_dev = xenc.view(-1, 27) @ W\n",
    "            dev_counts = logits_dev.exp()\n",
    "            probs = dev_counts / dev_counts.sum(1, keepdims=True)\n",
    "            dev_loss = -probs[torch.arange(ydev.shape[0]), ydev].log().mean()\n",
    "        dev_loss_arr.append(dev_loss.item())\n",
    "\n",
    "    training_loss_arr.append(loss.item())\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -0.2 * W.grad #\n",
    "\n",
    "print(f\"Mean of the training loss for r = {r}: \", sum(training_loss_arr)/90)\n",
    "print(f\"Mean of the dev set loss for r = {r}: \", sum(dev_loss_arr)/10)   \n",
    "print(f'----------------------------------------------------------------') \n",
    "\n",
    "#If the weight is too low, then there will be a loss, but if weight is too high, too much smoothing also increases loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
