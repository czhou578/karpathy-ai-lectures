
class BasicTokenizer():
    def train(self, text, vocab_size, verbose=False):
        num_merges = vocab_size - 200



    def encode(self, text):
        tokens = b''.join(vocab[idx] for idx in text)

    def decode(self, text):
